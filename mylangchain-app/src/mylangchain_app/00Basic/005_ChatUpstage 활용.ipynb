{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add pypdf=\">=4.2.0,<5.0.0\"\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "- `.env` íŒŒì¼ì— `UPSTAGE_API_KEY` ë“±ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_X\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LLM ë‹µë³€ ìƒì„±\n",
    "\n",
    "- Upstage Consoleì—ì„œ ë°œê¸‰ë°›ì€ API Keyë¥¼ `UPSTAGE_API_KEY`ë¼ê³  ì €ì¥í•˜ë©´ ë³„ë„ì˜ ì„¤ì • ì—†ì´ `ChatUpstage`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "#llm = ChatUpstage(temperature=0.5)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "**LangChain**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì„ í™œìš©í•´ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” **ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤. LLMì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ì— í†µí•©í•  ë•Œ ë°œìƒí•˜ëŠ” ë³µì¡ì„±(ì˜ˆ: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬, ë„êµ¬ ì—°ë™, ë©”ëª¨ë¦¬ ì²˜ë¦¬ ë“±)ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ğŸ“Œ **LangChainì˜ í•µì‹¬ ê°œë…**\n",
      "1. **ëª¨ë“ˆì‹ ì„¤ê³„**  \n",
      "   - LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬, ê²€ìƒ‰, ì²´ì¸(Chain), ì—ì´ì „íŠ¸(Agent) ë“±ì„ ì¡°í•©í•´ ìœ ì—°í•˜ê²Œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - ì˜ˆ: `Prompt Template + LLM + Retrieval`ì„ ì—°ê²°í•´ QA ì‹œìŠ¤í…œ ìƒì„±.\n",
      "\n",
      "2. **ì£¼ìš” êµ¬ì„± ìš”ì†Œ**  \n",
      "   - **ëª¨ë¸ I/O**: OpenAI, Anthropic, HuggingFace ë“± ë‹¤ì–‘í•œ LLMê³¼ í˜¸í™˜.\n",
      "   - **ì²´ì¸(Chain)**: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•´ ë³µì¡í•œ ì‘ì—… ì²˜ë¦¬ (ì˜ˆ: ìš”ì•½ â†’ ë²ˆì—­ â†’ ì €ì¥).\n",
      "   - **ì—ì´ì „íŠ¸(Agent)**: LLMì´ ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•´ ì‘ì—… ìˆ˜í–‰ (ì˜ˆ: ê³„ì‚°ê¸°, ê²€ìƒ‰ ì—”ì§„ í˜¸ì¶œ).\n",
      "   - **ë©”ëª¨ë¦¬(Memory)**: ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•´ ì»¨í…ìŠ¤íŠ¸ ì§€ì† (ì±—ë´‡ì— ìœ ìš©).\n",
      "   - **ë°ì´í„° ì—°ê²°**: ë²¡í„° DB(Pinecone, FAISS), API, SQL ë“±ê³¼ ì—°ë™.\n",
      "\n",
      "3. **ì‚¬ìš© ì‚¬ë¡€**  \n",
      "   - **ì±—ë´‡**: ëŒ€í™” ê¸°ë¡ê³¼ ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ ê²°í•©.\n",
      "   - **ìë™í™”ëœ ë¬¸ì„œ ì²˜ë¦¬**: PDF ë¶„ì„ â†’ ìš”ì•½ â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥.\n",
      "   - **ì½”ë“œ ìƒì„±**: GitHub APIì™€ ì—°ë™í•´ ì½”ë“œ ì¶”ì²œ.\n",
      "\n",
      "### ğŸ›  **LangChainì˜ ì¥ì **\n",
      "- **ìƒì‚°ì„± í–¥ìƒ**: ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ì„ ì½”ë“œë¡œ ê°„ë‹¨íˆ êµ¬í˜„.\n",
      "- **í™•ì¥ì„±**: ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì¶”ê°€ ê°€ëŠ¥ (ì˜ˆ: ìì²´ LLM í†µí•©).\n",
      "- **ì»¤ë®¤ë‹ˆí‹° ì§€ì›**: í™œë°œí•œ ìƒíƒœê³„ë¡œ íŠœí† ë¦¬ì–¼ê³¼ ì˜ˆì œ í’ë¶€.\n",
      "\n",
      "### ğŸ“¦ **ê¸°ìˆ  ìŠ¤íƒ ì˜ˆì‹œ**\n",
      "```python\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "# 1. ë²¡í„° DB ë¡œë“œ\n",
      "embedding = OpenAIEmbeddings()\n",
      "db = Chroma(persist_directory='./docs', embedding_function=embedding)\n",
      "\n",
      "# 2. LLM + ê²€ìƒ‰ ì²´ì¸ ìƒì„±\n",
      "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=db.as_retriever())\n",
      "\n",
      "# 3. ì§ˆë¬¸ ì‘ë‹µ\n",
      "result = qa.run(\"2023ë…„ ë§¤ì¶œì€ ì–¼ë§ˆì¸ê°€ìš”?\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "### ğŸ” **ì°¸ê³  ìë£Œ**\n",
      "- ê³µì‹ ë¬¸ì„œ: [https://python.langchain.com](https://python.langchain.com)\n",
      "- GitHub: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)\n",
      "\n",
      "LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ **\"ë ˆê³  ì¡°ë¦½\"** í•˜ë“¯ì´ ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ë¡œ, íŠ¹íˆ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ê°€ í•„ìš”í•œ ê²½ìš° ìœ ìš©í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "ai_message=llm.invoke(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(type(ai_message))\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chat stream\n",
    "for chunk in llm.stream(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a professional translator specializing in Korean-English translation.\"),\n",
    "        (\"human\", \"Translate this from {source_lang} to {target_lang}: {text}\")\n",
    "    ])\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "chain = translation_prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"source_lang\": \"English\",\n",
    "    \"target_lang\": \"Korean\", \n",
    "    \"text\": \"LangChain is a powerful framework for building AI applications.\"\n",
    "})\n",
    "\n",
    "print(\"Upstage Response:\")\n",
    "print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# using chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to Korean.\"),\n",
    "        (\"human\", \"Translate this sentence from English to Korean. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_message=chain.invoke({\"english_text\": \"Hello, How are you?\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groundness Check\n",
    "Groundedness Check APIëŠ” ì‚¬ìš©ìê°€ ì œê³µí•œ Context(ì»¨í…ìŠ¤íŠ¸)ì— ëŒ€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì´ ì‹¤ì œë¡œ ê·¸ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounded\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "request_input = {\n",
    "    \"context\": \"ì‚¼ì„±ì „ìëŠ” ì—°ê²° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œ 74.07ì¡°ì›, ì˜ì—…ì´ìµ 10.44ì¡°ì›ì˜ 2024ë…„ 2ë¶„ê¸° ì‹¤ì ì„ ë°œí‘œí–ˆë‹¤. ì „ì‚¬ ë§¤ì¶œì€ ì „ë¶„ê¸° ëŒ€ë¹„ 3% ì¦ê°€í•œ 74.07ì¡°ì›ì„ ê¸°ë¡í–ˆë‹¤. DSë¶€ë¬¸ì€ ë©”ëª¨ë¦¬ ì—…í™© íšŒë³µìœ¼ë¡œ ì „ë¶„ê¸° ëŒ€ë¹„ 23% ì¦ê°€í•˜ê³ , SDCëŠ” OLED íŒë§¤ í˜¸ì¡°ë¡œ ì¦ê°€í–ˆë‹¤.\",\n",
    "    \"answer\": \"ì‚¼ì„±ì „ìì˜ 2024ë…„ 2ë¶„ê¸° ë§¤ì¶œì€ ì•½ 74.07ì¡°ì›ì´ë‹¤.\",\n",
    "}\n",
    "\n",
    "response = groundedness_check.invoke(request_input)\n",
    "print(response)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UpstageEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "doc_result = embeddings.embed_documents(\n",
    "    [\"Sung is a professor.\", \"This is another document\"]\n",
    ")\n",
    "print(doc_result)\n",
    "\n",
    "query_result = embeddings.embed_query(\"What does Sung do?\")\n",
    "print(query_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
